**このフォルダには特定のソースに対応したクローラーを配置できます。ここでのクローラーはソースの記事リストURLを解析し、記事の詳細情報を辞書形式で返す必要があります。**
> 
> # カスタムクローラーの設定
> 
> クローラーを作成した後、そのプログラムをこのフォルダに配置し、`__init__.py` の scraper_map に次のように登録します：
> 
> ```python
> {'www.securityaffairs.com': securityaffairs_scraper}
> ```
> 
> ここで、キーはソースのURLで、値は関数名です。
> 
> クローラーは関数形式で記述し、以下の入力および出力仕様を満たす必要があります：
> 
> 入力：
> - expiration： `datetime.date` オブジェクト、クローラーはこの日付以降（この日を含む）の記事のみを取得する必要があります。
> - existings：[str]、データベースに既存する記事のURLリスト、クローラーはこのリスト内のURLを無視する必要があります。
> 
> 出力：
> - [dict]、結果の辞書リスト、各辞書は以下の形式で1つの記事を表します：
> `[{'url': str, 'title': str, 'author': str, 'publish_time': str, 'content': str, 'abstract': str, 'images': [Path]}, {...}, ...]`
> 
> 注意：`publish_time`の形式は`"%Y%m%d"`である必要があります。クローラーで取得できない場合は、当日の日付を使用できます。
> 
> さらに、`title`と`content`は必須フィールドです。
> 
> # 一般ページパーサー
> 
> ここでは一般的なページパーサーを提供しており、ソースから記事リストをインテリジェントに取得できます。各記事URLに対して、最初に gne を使用して解析を試みます。失敗した場合は、llm を使用して解析を試みます。
> 
> このソリューションにより、ほとんどの一般的なニュースおよびポータルソースのスキャンと情報抽出が可能になります。
> 
> **しかし、より理想的かつ効率的なスキャンを実現するために、ユーザー自身でカスタムクローラーを作成するか、直接弊社のデータサービスを購読することを強くお勧めします。**